{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'ann', 'a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\"an\": 1,\"ann\":2, \"a\":3}\n",
    "list(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train accuracy\n",
      "DEBUG:root:0.6931180454987903\n",
      "DEBUG:root:0.692069908391581\n",
      "DEBUG:root:0.6769690509736208\n",
      "DEBUG:root:0.5468116542529191\n",
      "DEBUG:root:0.3128771470519909\n",
      "INFO:root:1.0\n",
      "INFO:root:1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsgHNeGMnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a20d50b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "    \n",
    "\n",
    "def initialize_parameter(layer_dims: list) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        # Initialize in the accordance with He papers\n",
    "        # parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n",
    "\n",
    "def forward_function(prev_A, W, b):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    A = sigmoid(Z)    \n",
    "    \n",
    "    #test done\n",
    "    return Z, A\n",
    "    \n",
    "\n",
    "\n",
    "def l_layer_forward(X, parameters, layer_dims):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        Z, A = forward_function(prev_A, W, b)\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs\n",
    "\n",
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads \n",
    "\n",
    "def backward_function(dA, Z, A, prev_A, W, m, lambd):\n",
    "    # limited sigmoid\n",
    "    dZ = dA * A * (1 - A)\n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A\n",
    "\n",
    "def update_parameters(parameters, grads, layer_dims, learning_rate):\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "\n",
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=100, lambd=0.1):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    parameters = initialize_parameter(layer_dims)\n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    m = X.shape[1]\n",
    "    costs = np.array([])\n",
    "    for i in range(iteration_num):\n",
    "        outputs = {}\n",
    "        outputs = l_layer_forward(X, parameters, layer_dims)\n",
    "        cost=caluculate_cost(outputs[str_AL],y,m,lambd,parameters)\n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        grads = {}\n",
    "        grads = l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, layer_dims, learning_rate)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            logger.debug(cost)\n",
    "            \n",
    "    plt.title(\"Cost Function\")\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "    \n",
    "def predict(X, parameters, layer_dims):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims)\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score\n",
    "    \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    logger.info(\"train accuracy\")\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=5000, lambd=0.3)\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims)\n",
    "    logger.info(caluculate_score(y_train, y_train_hat))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(caluculate_score(y_test, y_test_hat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
