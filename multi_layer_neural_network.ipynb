{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  1.25],\n",
       "       [ 2.5 ,  3.75],\n",
       "       [ 5.  ,  6.25]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.random.rand(3, 2)\n",
    "D = (D <  0.8)\n",
    "A = np.arange(6).reshape(3,2)\n",
    "A = D * A\n",
    "A / 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"sigmoid\"\n",
    "            keep_probs = 1\n",
    "            \n",
    "        Z, A = forward_function(prev_A, W, b, activation)    \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, v, S):\n",
    "    \n",
    "    if learning_rate_decay:\n",
    "        learning_rate = (0.99999999 ** iteration_i) * learning_rate\n",
    "        \n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.000001\n",
    "        \n",
    "    if optimize==\"gd\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"momentum\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * v[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * v[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"rmsprop\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)] / np.sqrt(S[\"dW\" + str(i)] + epsilon)\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * S[\"db\" + str(i)] / np.sqrt(S[\"db\" + str(i)] + epsilon)\n",
    "        \n",
    "    elif optimize == \"adam\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            \n",
    "            # adjust the scale of each values\n",
    "            vdW_correction = v[\"dW\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            vdb_correction = v[\"db\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            SdW_correction = S[\"dW\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            Sdb_correction = S[\"db\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            \n",
    "            parameters[\"W\" + str(i)] -= learning_rate * vdW_correction / np.sqrt(SdW_correction + epsilon) \n",
    "            parameters[\"b\" + str(i)] -= learning_rate * vdb_correction / np.sqrt(Sdb_correction + epsilon)\n",
    "            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_momentum(parameters, optimize):\n",
    "    optimize_for_momentums = {}\n",
    "    if optimize == \"gd\" or optimize == \"rmsprop\":\n",
    "        pass\n",
    "    \n",
    "    elif optimize == \"momentum\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_momentums[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "    \n",
    "    \n",
    "    return optimize_for_momentums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_rmsprop(parameters, optimize):\n",
    "    optimize_for_rmsprops = {}\n",
    "    if optimize == \"sgd\" or optimize == \"momentum\":\n",
    "        pass\n",
    "    elif optimize == \"rmsprop\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_rmsprops[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "            \n",
    "    return optimize_for_rmsprops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, \\\n",
    "         activation=\"relu\", initialize=\"random\", minibatch_size=None, learning_rate_decay=False, \\\n",
    "        optimize=\"gd\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    \n",
    "    optimize_for_momentums = initialize_for_momentum(parameters, optimize)\n",
    "    \n",
    "    optimize_for_rmsprops = initialize_for_rmsprop(parameters, optimize)\n",
    "        \n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for iteration_i in range(1, iteration_num + 1):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            \n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "            \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, optimize_for_momentums, optimize_for_rmsprops)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if iteration_i % 1000 == 0:\n",
    "            logger.info(cost)\n",
    "            \n",
    "    plt.title(\"{}\".format(optimize))\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"relu\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:1\n",
      "INFO:root:0.030873655841662753\n",
      "INFO:root:0.030430696763533327\n",
      "INFO:root:0.0303408079074482\n",
      "INFO:root:0.03032023362675901\n",
      "INFO:root:0.030314429389283463\n",
      "INFO:root:0.0303114102067704\n",
      "INFO:root:0.03031077452174503\n",
      "INFO:root:0.03031047276875853\n",
      "INFO:root:0.030310321453718973\n",
      "INFO:root:0.030310290417267897\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 1.0\n",
      "DEBUG:root:1\n",
      "INFO:root:0.3364754722857738\n",
      "INFO:root:0.13044803166081742\n",
      "INFO:root:0.08522169389951469\n",
      "INFO:root:0.06763850069131451\n",
      "INFO:root:0.058690373805386796\n",
      "INFO:root:0.05335854797275455\n",
      "INFO:root:0.04988751876487816\n",
      "INFO:root:0.047463604162002675\n",
      "INFO:root:0.04566409816413879\n",
      "INFO:root:0.044272831357898806\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 1.0\n",
      "DEBUG:root:1\n",
      "INFO:root:0.6931471806598064\n",
      "INFO:root:0.6931471807504204\n",
      "INFO:root:0.6931471807976224\n",
      "INFO:root:0.6931471808249161\n",
      "INFO:root:0.6931471808452921\n",
      "INFO:root:0.6931471808640514\n",
      "INFO:root:0.6931471808832698\n",
      "INFO:root:0.6931471809038271\n",
      "INFO:root:0.6931471809261884\n",
      "INFO:root:0.6931471809506942\n",
      "INFO:root:train accuracy 0.5\n",
      "INFO:root:test accuracy 0.5\n",
      "DEBUG:root:1\n",
      "INFO:root:0.09069400958487\n",
      "INFO:root:0.037054023930842984\n",
      "INFO:root:0.03412001688959647\n",
      "INFO:root:0.033554938072502635\n",
      "INFO:root:0.03338464808219155\n",
      "INFO:root:0.03330737995489858\n",
      "INFO:root:0.03325604404081309\n",
      "INFO:root:0.033213027631570635\n",
      "INFO:root:0.03317327729573068\n",
      "INFO:root:0.03313524919932058\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxowAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1e62a9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5,  optimize=\"adam\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5, optimize=\"gd\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims,  activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5,   optimize=\"rmsprop\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5, optimize=\"momentum\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
