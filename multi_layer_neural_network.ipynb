{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"sigmoid\"\n",
    "        Z, A = forward_function(prev_A, W, b, activation)\n",
    "        \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, v, S):\n",
    "    \n",
    "    if learning_rate_decay:\n",
    "        learning_rate = (0.99999999 ** iteration_i) * learning_rate\n",
    "        \n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.000001\n",
    "        \n",
    "    if optimize==\"gd\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"momentum\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * v[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * v[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"rmsprop\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)] / np.sqrt(S[\"dW\" + str(i)] + epsilon)\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * S[\"db\" + str(i)] / np.sqrt(S[\"db\" + str(i)] + epsilon)\n",
    "        \n",
    "    elif optimize == \"adam\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            \n",
    "            # adjust the scale of each values\n",
    "            vdW_correction = v[\"dW\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            vdb_correction = v[\"db\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            SdW_correction = S[\"dW\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            Sdb_correction = S[\"db\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            \n",
    "            parameters[\"W\" + str(i)] -= learning_rate * vdW_correction / np.sqrt(SdW_correction + epsilon) \n",
    "            parameters[\"b\" + str(i)] -= learning_rate * vdb_correction / np.sqrt(Sdb_correction + epsilon)\n",
    "            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_momentum(parameters, optimize):\n",
    "    optimize_for_momentums = {}\n",
    "    if optimize == \"gd\" or optimize == \"rmsprop\":\n",
    "        pass\n",
    "    \n",
    "    elif optimize == \"momentum\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_momentums[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "    \n",
    "    \n",
    "    return optimize_for_momentums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_rmsprop(parameters, optimize):\n",
    "    optimize_for_rmsprops = {}\n",
    "    if optimize == \"sgd\" or optimize == \"momentum\":\n",
    "        pass\n",
    "    elif optimize == \"rmsprop\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_rmsprops[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "            \n",
    "    return optimize_for_rmsprops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, \\\n",
    "         activation=\"relu\", initialize=\"random\", minibatch_size=None, learning_rate_decay=False, \\\n",
    "        optimize=\"gd\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    \n",
    "    optimize_for_momentums = initialize_for_momentum(parameters, optimize)\n",
    "    \n",
    "    optimize_for_rmsprops = initialize_for_rmsprop(parameters, optimize)\n",
    "        \n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for iteration_i in range(1, iteration_num + 1):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "        \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, optimize_for_momentums, optimize_for_rmsprops)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if iteration_i % 100 == 0:\n",
    "            logger.info(cost)\n",
    "            \n",
    "    plt.title(\"{}\".format(optimize))\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"tanh\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:1\n",
      "INFO:root:0.04175551623801178\n",
      "INFO:root:0.03501518497120067\n",
      "INFO:root:0.03319063156505845\n",
      "INFO:root:0.03221005096904187\n",
      "INFO:root:0.031616789756737845\n",
      "INFO:root:0.031240024703806383\n",
      "INFO:root:0.03099413235883461\n",
      "INFO:root:0.030830502313755045\n",
      "INFO:root:0.030715568444974407\n",
      "INFO:root:0.030629730889689077\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 0.95\n",
      "DEBUG:root:1\n",
      "INFO:root:0.6929810955866977\n",
      "INFO:root:0.6925335011190842\n",
      "INFO:root:0.6912299128752912\n",
      "INFO:root:0.6872540470009862\n",
      "INFO:root:0.6755143620432599\n",
      "INFO:root:0.6440908178299807\n",
      "INFO:root:0.5779903077066955\n",
      "INFO:root:0.4859944715651279\n",
      "INFO:root:0.40221396753165134\n",
      "INFO:root:0.3400381054594415\n",
      "INFO:root:train accuracy 0.9875\n",
      "INFO:root:test accuracy 1.0\n",
      "DEBUG:root:1\n",
      "INFO:root:0.6931471806100202\n",
      "INFO:root:0.6931471805599458\n",
      "INFO:root:0.6931471805599463\n",
      "INFO:root:0.6931471805599468\n",
      "INFO:root:0.6931471805599476\n",
      "INFO:root:0.6931471805599483\n",
      "INFO:root:0.693147180559949\n",
      "INFO:root:0.6931471805599497\n",
      "INFO:root:0.6931471805599504\n",
      "INFO:root:0.693147180559951\n",
      "INFO:root:train accuracy 0.5\n",
      "INFO:root:test accuracy 0.5\n",
      "DEBUG:root:1\n",
      "INFO:root:0.6924800279296808\n",
      "INFO:root:0.6902393156965918\n",
      "INFO:root:0.6813189238017303\n",
      "INFO:root:0.6518834468432774\n",
      "INFO:root:0.5772111304501436\n",
      "INFO:root:0.46033839589527975\n",
      "INFO:root:0.3688382646865094\n",
      "INFO:root:0.30861417502930516\n",
      "INFO:root:0.2665675219734802\n",
      "INFO:root:0.23442091105932933\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgp3EZEU+v8s1tL4Q1InhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ec38eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, keep_probs=1,  optimize=\"adam\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, keep_probs=1,  optimize=\"gd\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, keep_probs=1,  optimize=\"rmsprop\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, keep_probs=1,  optimize=\"momentum\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
