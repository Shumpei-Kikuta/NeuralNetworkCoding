{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"sigmoid\"\n",
    "        Z, A = forward_function(prev_A, W, b, activation)\n",
    "        \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate0, learning_rate_decay, iteration_i, optimize, v, S):\n",
    "    \n",
    "    if learning_rate_decay:\n",
    "        learning_rate = (0.99999999 ** iteration_i) * learning_rate0\n",
    "        \n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.9\n",
    "    epsilon = 0.0001\n",
    "        \n",
    "    if optimize==\"gd\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"momentum\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * v[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * v[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"rmsprop\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)] / np.sqrt(S[\"dW\" + str(i)] + epsilon)\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * S[\"db\" + str(i)] / np.sqrt(S[\"db\" + str(i)] + epsilon)\n",
    "        \n",
    "    elif optimize == \"adam\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            \n",
    "            # adjust the scale of each values\n",
    "            vdW_correction = v[\"dW\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            vdb_correction = v[\"db\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            SdW_correction = S[\"dW\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            Sdb_correction = S[\"db\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            \n",
    "            parameters[\"W\" + str(i)] -= learning_rate * vdW_correction / np.sqrt(SdW_correction + epsilon) \n",
    "            parameters[\"b\" + str(i)] -= learning_rate * vdb_correction / np.sqrt(Sdb_correction + epsilon)\n",
    "            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_momentum(parameters, optimize):\n",
    "    optimize_for_momentums = {}\n",
    "    if optimize == \"gd\" or optimize == \"rmsprop\":\n",
    "        pass\n",
    "    \n",
    "    elif optimize == \"momentum\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_momentums[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "    \n",
    "    \n",
    "    return optimize_for_momentums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_rmsprop(parameters, optimize):\n",
    "    optimize_for_rmsprops = {}\n",
    "    if optimize == \"sgd\" or optimize == \"momentum\":\n",
    "        pass\n",
    "    elif optimize == \"rmsprop\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_rmsprops[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "            \n",
    "    return optimize_for_rmsprops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, \\\n",
    "         activation=\"relu\", initialize=\"random\", minibatch_size=None, learning_rate_decay=False, \\\n",
    "        optimize=\"gd\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    \n",
    "    optimize_for_momentums = initialize_for_momentum(parameters, optimize)\n",
    "    \n",
    "    optimize_for_rmsprops = initialize_for_rmsprop(parameters, optimize)\n",
    "        \n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for i in range(iteration_num):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "        \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, i, optimize, optimize_for_momentums, optimize_for_rmsprops)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            logger.info(cost)\n",
    "            \n",
    "    plt.title(\"Cost Function\")\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"tanh\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train accuracy\n",
      "DEBUG:root:1\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:37: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:38: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:38: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:39: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:40: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "INFO:root:0.6931469714159199\n",
      "/Users/shumpei/Desktop/ai_specialization_course/activation_helper.py:9: RuntimeWarning: invalid value encountered in maximum\n",
      "  return np.maximum(0, x)\n",
      "/Users/shumpei/Desktop/ai_specialization_course/activation_helper.py:5: RuntimeWarning: invalid value encountered in greater\n",
      "  return np.where(x > 0, 1. / (1. + np.exp(-x)), np.exp(x) / (np.exp(x) + np.exp(0)))\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in greater\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:train accuracy 0.5\n",
      "INFO:root:test accuracy 0.5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nECRJjYEgSQIMBElS8/8Bwseqm5zMlBUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aaa5e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    logger.info(\"train accuracy\")\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, keep_probs=1, learning_rate_decay=True, optimize=\"adam\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1, activation=\"relu\") #\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
