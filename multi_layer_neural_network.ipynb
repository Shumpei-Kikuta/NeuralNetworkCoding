{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z)\n",
    "    sum_Z = np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    assert(sum_Z.shape == (1, exp_Z.shape[1]))\n",
    "    return exp_Z / sum_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"softmax\"\n",
    "            keep_probs = 1\n",
    "            \n",
    "        Z, A = forward_function(prev_A, W, b, activation)    \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1/m * np.sum(y *np.log(AL)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    assert(y.shape == AL.shape)\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, v, S):\n",
    "    \n",
    "    if learning_rate_decay:\n",
    "        learning_rate = (0.99999999 ** iteration_i) * learning_rate\n",
    "        \n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.000001\n",
    "        \n",
    "    if optimize==\"gd\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"momentum\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * v[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * v[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"rmsprop\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)] / np.sqrt(S[\"dW\" + str(i)] + epsilon)\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * S[\"db\" + str(i)] / np.sqrt(S[\"db\" + str(i)] + epsilon)\n",
    "        \n",
    "    elif optimize == \"adam\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            \n",
    "            # adjust the scale of each values\n",
    "            vdW_correction = v[\"dW\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            vdb_correction = v[\"db\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            SdW_correction = S[\"dW\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            Sdb_correction = S[\"db\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            \n",
    "            parameters[\"W\" + str(i)] -= learning_rate * vdW_correction / np.sqrt(SdW_correction + epsilon) \n",
    "            parameters[\"b\" + str(i)] -= learning_rate * vdb_correction / np.sqrt(Sdb_correction + epsilon)\n",
    "            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_momentum(parameters, optimize):\n",
    "    optimize_for_momentums = {}\n",
    "    if optimize == \"gd\" or optimize == \"rmsprop\":\n",
    "        pass\n",
    "    \n",
    "    elif optimize == \"momentum\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_momentums[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "    \n",
    "    \n",
    "    return optimize_for_momentums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_rmsprop(parameters, optimize):\n",
    "    optimize_for_rmsprops = {}\n",
    "    if optimize == \"sgd\" or optimize == \"momentum\":\n",
    "        pass\n",
    "    elif optimize == \"rmsprop\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_rmsprops[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "            \n",
    "    return optimize_for_rmsprops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, \\\n",
    "         activation=\"relu\", initialize=\"he\", minibatch_size=None, learning_rate_decay=False, \\\n",
    "        optimize=\"gd\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    \n",
    "    optimize_for_momentums = initialize_for_momentum(parameters, optimize)\n",
    "    \n",
    "    optimize_for_rmsprops = initialize_for_rmsprop(parameters, optimize)\n",
    "        \n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for iteration_i in range(1, iteration_num + 1):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            \n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "            \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, optimize_for_momentums, optimize_for_rmsprops)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if iteration_i % 100 == 0:\n",
    "            logger.info(cost)\n",
    "#             print(outputs[\"A2\"])\n",
    "            \n",
    "    plt.title(\"{}\".format(optimize))\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"relu\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    for i in range(X.shape[1]) :\n",
    "        k = np.argmax(y_hat[:, i])\n",
    "        tmp = np.zeros((y_hat.shape[0]))\n",
    "        tmp[k] = 1\n",
    "        y_hat[:, i] = tmp\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y_hat.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / (y.shape[1] * y.shape[0])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:1\n",
      "INFO:root:0.420663621452\n",
      "INFO:root:0.279239232464\n",
      "INFO:root:0.211158713496\n",
      "INFO:root:0.17350029792\n",
      "INFO:root:0.153098233716\n",
      "INFO:root:0.141515933575\n",
      "INFO:root:0.13420704473\n",
      "INFO:root:0.129252338579\n",
      "INFO:root:0.125672981488\n",
      "INFO:root:0.122939499219\n",
      "INFO:root:train accuracy 0.9833333333333333\n",
      "INFO:root:test accuracy 1.0\n",
      "DEBUG:root:1\n",
      "INFO:root:1.07612286364\n",
      "INFO:root:1.03553045446\n",
      "INFO:root:0.966629487034\n",
      "INFO:root:0.873187625017\n",
      "INFO:root:0.788305509303\n",
      "INFO:root:0.727832012302\n",
      "INFO:root:0.686079448987\n",
      "INFO:root:0.655873449668\n",
      "INFO:root:0.632914322744\n",
      "INFO:root:0.615209915788\n",
      "INFO:root:0.601102264924\n",
      "INFO:root:0.589632497611\n",
      "INFO:root:0.58024340119\n",
      "INFO:root:0.572430056567\n",
      "INFO:root:0.565839463458\n",
      "INFO:root:0.560210778876\n",
      "INFO:root:0.555349771142\n",
      "INFO:root:0.551110866176\n",
      "INFO:root:0.547384583055\n",
      "INFO:root:0.544083032209\n",
      "INFO:root:0.541136962907\n",
      "INFO:root:0.538492489669\n",
      "INFO:root:0.536106248119\n",
      "INFO:root:0.533942628111\n",
      "INFO:root:0.531972112385\n",
      "INFO:root:0.53017003969\n",
      "INFO:root:0.528515657236\n",
      "INFO:root:0.526991385477\n",
      "INFO:root:0.525582241639\n",
      "INFO:root:0.524275383439\n",
      "INFO:root:0.523059744824\n",
      "INFO:root:0.521925742875\n",
      "INFO:root:0.520865040257\n",
      "INFO:root:0.519870351373\n",
      "INFO:root:0.518935283206\n",
      "INFO:root:0.518054203853\n",
      "INFO:root:0.517222133347\n",
      "INFO:root:0.516434652502\n",
      "INFO:root:0.515687973934\n",
      "INFO:root:0.514979672257\n",
      "INFO:root:0.514306875256\n",
      "INFO:root:0.513666478418\n",
      "INFO:root:0.513056311774\n",
      "INFO:root:0.512474197221\n",
      "INFO:root:0.511918287215\n",
      "INFO:root:0.511386373617\n",
      "INFO:root:0.510877380824\n",
      "INFO:root:0.5103896046\n",
      "INFO:root:0.509921122964\n",
      "INFO:root:0.509471403501\n",
      "INFO:root:0.509039000507\n",
      "INFO:root:0.508622943383\n",
      "INFO:root:0.508222243892\n",
      "INFO:root:0.507836075596\n",
      "INFO:root:0.507463383187\n",
      "INFO:root:0.507102281672\n",
      "INFO:root:0.506751252756\n",
      "INFO:root:0.506409300942\n",
      "INFO:root:0.506075575514\n",
      "INFO:root:0.505749286861\n",
      "INFO:root:0.505429686578\n",
      "INFO:root:0.505116059753\n",
      "INFO:root:0.504807720118\n",
      "INFO:root:0.504504006281\n",
      "INFO:root:0.504204278665\n",
      "INFO:root:0.503907917047\n",
      "INFO:root:0.503614318681\n",
      "INFO:root:0.503322896998\n",
      "INFO:root:0.50303535087\n",
      "INFO:root:0.502757920651\n",
      "INFO:root:0.502486804042\n",
      "INFO:root:0.502216465574\n",
      "INFO:root:0.501944436306\n",
      "INFO:root:0.501669488876\n",
      "INFO:root:0.501390624221\n",
      "INFO:root:0.501106877829\n",
      "INFO:root:0.500817287659\n",
      "INFO:root:0.500529657253\n",
      "INFO:root:0.500233101492\n",
      "INFO:root:0.499923049324\n",
      "INFO:root:0.499593987387\n",
      "INFO:root:0.49921332977\n",
      "INFO:root:0.498705280185\n",
      "INFO:root:0.497936598853\n",
      "INFO:root:0.496217257795\n",
      "INFO:root:0.491822240493\n",
      "INFO:root:0.481475397005\n",
      "INFO:root:0.461793807907\n",
      "INFO:root:0.437582052941\n",
      "INFO:root:0.413086891519\n",
      "INFO:root:0.390638022002\n",
      "INFO:root:0.371700556705\n",
      "INFO:root:0.356083879269\n",
      "INFO:root:0.342638881628\n",
      "INFO:root:0.330820038921\n",
      "INFO:root:0.32029078661\n",
      "INFO:root:0.31083323533\n",
      "INFO:root:0.302295141444\n",
      "INFO:root:0.29455771422\n",
      "INFO:root:0.287522131767\n",
      "INFO:root:train accuracy 0.9777777777777777\n",
      "INFO:root:test accuracy 1.0\n",
      "DEBUG:root:1\n",
      "INFO:root:0.361581340422\n",
      "INFO:root:0.219696007193\n",
      "INFO:root:0.197152719671\n",
      "INFO:root:0.192067783659\n",
      "INFO:root:0.166367531542\n",
      "INFO:root:0.163223008458\n",
      "INFO:root:0.160103459836\n",
      "INFO:root:0.157241656572\n",
      "INFO:root:0.156747189964\n",
      "INFO:root:0.156077778063\n",
      "INFO:root:0.155719385581\n",
      "INFO:root:0.155612751472\n",
      "INFO:root:0.155520023674\n",
      "INFO:root:0.155854368757\n",
      "INFO:root:0.15600962391\n",
      "INFO:root:0.156002935369\n",
      "INFO:root:0.156605447136\n",
      "INFO:root:0.157075175689\n",
      "INFO:root:0.157412999114\n",
      "INFO:root:0.157911380008\n",
      "INFO:root:0.158365744267\n",
      "INFO:root:0.158777865138\n",
      "INFO:root:0.159382476032\n",
      "INFO:root:0.159594581931\n",
      "INFO:root:0.160320124736\n",
      "INFO:root:0.160975627217\n",
      "INFO:root:0.161508264204\n",
      "INFO:root:0.162005315247\n",
      "INFO:root:0.162407899588\n",
      "INFO:root:0.162981343051\n",
      "INFO:root:0.163630968254\n",
      "INFO:root:0.164302848683\n",
      "INFO:root:0.165023023363\n",
      "INFO:root:0.165487812099\n",
      "INFO:root:0.166211668841\n",
      "INFO:root:0.16658238159\n",
      "INFO:root:0.167035997817\n",
      "INFO:root:0.167681148893\n",
      "INFO:root:0.168132351405\n",
      "INFO:root:0.168601227373\n",
      "INFO:root:0.169029956973\n",
      "INFO:root:0.169468711795\n",
      "INFO:root:0.169733928219\n",
      "INFO:root:0.170100039713\n",
      "INFO:root:0.170585268302\n",
      "INFO:root:0.170826370454\n",
      "INFO:root:0.171117362043\n",
      "INFO:root:0.171388154398\n",
      "INFO:root:0.171592157963\n",
      "INFO:root:0.171995985167\n",
      "INFO:root:0.172046566144\n",
      "INFO:root:0.172376130139\n",
      "INFO:root:0.1725230327\n",
      "INFO:root:0.172564840248\n",
      "INFO:root:0.172539080552\n",
      "INFO:root:0.172646667755\n",
      "INFO:root:0.172687715704\n",
      "INFO:root:0.172728140705\n",
      "INFO:root:0.172807925229\n",
      "INFO:root:0.173080999584\n",
      "INFO:root:0.172955352485\n",
      "INFO:root:0.172782613802\n",
      "INFO:root:0.172636184962\n",
      "INFO:root:0.172494080978\n",
      "INFO:root:0.172348988647\n",
      "INFO:root:0.172193207139\n",
      "INFO:root:0.172020940167\n",
      "INFO:root:0.171829386327\n",
      "INFO:root:0.171618321725\n",
      "INFO:root:0.171512987228\n",
      "INFO:root:0.171207207524\n",
      "INFO:root:0.170909394754\n",
      "INFO:root:0.170736627919\n",
      "INFO:root:0.170395378829\n",
      "INFO:root:0.170030830165\n",
      "INFO:root:0.169779209607\n",
      "INFO:root:0.169449615533\n",
      "INFO:root:0.169090822627\n",
      "INFO:root:0.168768878061\n",
      "INFO:root:0.168370129453\n",
      "INFO:root:0.168010446301\n",
      "INFO:root:0.167641381552\n",
      "INFO:root:0.167257449156\n",
      "INFO:root:0.166864708514\n",
      "INFO:root:0.166467149633\n",
      "INFO:root:0.16606588039\n",
      "INFO:root:0.165661112977\n",
      "INFO:root:0.165253128561\n",
      "INFO:root:0.164842360804\n",
      "INFO:root:0.164429290967\n",
      "INFO:root:0.164014397744\n",
      "INFO:root:0.163598150409\n",
      "INFO:root:0.163181009689\n",
      "INFO:root:0.162763426687\n",
      "INFO:root:0.162345840953\n",
      "INFO:root:0.161928678874\n",
      "INFO:root:0.161512352519\n",
      "INFO:root:0.161097258783\n",
      "INFO:root:0.160683778769\n",
      "INFO:root:0.160272277337\n",
      "INFO:root:train accuracy 0.9722222222222222\n",
      "INFO:root:test accuracy 1.0\n",
      "DEBUG:root:1\n",
      "INFO:root:1.11368231275\n",
      "INFO:root:1.04740163679\n",
      "INFO:root:0.848784883367\n",
      "INFO:root:0.720336521036\n",
      "INFO:root:0.637094612893\n",
      "INFO:root:0.589474525583\n",
      "INFO:root:0.562933760436\n",
      "INFO:root:0.547005641387\n",
      "INFO:root:0.536978387108\n",
      "INFO:root:0.530187569303\n",
      "INFO:root:0.525309590828\n",
      "INFO:root:0.521629872696\n",
      "INFO:root:0.518732569853\n",
      "INFO:root:0.516365377817\n",
      "INFO:root:0.514380394095\n",
      "INFO:root:0.512664225218\n",
      "INFO:root:0.51113667148\n",
      "INFO:root:0.509747184741\n",
      "INFO:root:0.508457768658\n",
      "INFO:root:0.50724204731\n",
      "INFO:root:0.506082422268\n",
      "INFO:root:0.504964125499\n",
      "INFO:root:0.503874923983\n",
      "INFO:root:0.50279520648\n",
      "INFO:root:0.501703715476\n",
      "INFO:root:0.500587634044\n",
      "INFO:root:0.499450673111\n",
      "INFO:root:0.498244276766\n",
      "INFO:root:0.496939778512\n",
      "INFO:root:0.495391673893\n",
      "INFO:root:0.492913480969\n",
      "INFO:root:0.484776393235\n",
      "INFO:root:0.454489556264\n",
      "INFO:root:0.416141098887\n",
      "INFO:root:0.382446416097\n",
      "INFO:root:0.355533749988\n",
      "INFO:root:0.331962863022\n",
      "INFO:root:0.310941553606\n",
      "INFO:root:0.292322593652\n",
      "INFO:root:0.275971960566\n",
      "INFO:root:0.261686754601\n",
      "INFO:root:0.249261688442\n",
      "INFO:root:0.238469873845\n",
      "INFO:root:0.229070032168\n",
      "INFO:root:0.22085850701\n",
      "INFO:root:0.213663990002\n",
      "INFO:root:0.207350583803\n",
      "INFO:root:0.201779847653\n",
      "INFO:root:0.19683034893\n",
      "INFO:root:0.192412860759\n",
      "INFO:root:0.188452562782\n",
      "INFO:root:0.184886576822\n",
      "INFO:root:0.181662031607\n",
      "INFO:root:0.178735573553\n",
      "INFO:root:0.176078965486\n",
      "INFO:root:0.173651281581\n",
      "INFO:root:0.171424223231\n",
      "INFO:root:0.169374320255\n",
      "INFO:root:0.167481538159\n",
      "INFO:root:0.165728689202\n",
      "INFO:root:0.164100957976\n",
      "INFO:root:0.162585515589\n",
      "INFO:root:0.161171204334\n",
      "INFO:root:0.159848278745\n",
      "INFO:root:0.158608191984\n",
      "INFO:root:0.157443418866\n",
      "INFO:root:0.156347308605\n",
      "INFO:root:0.155314137435\n",
      "INFO:root:0.154341104372\n",
      "INFO:root:0.153421008529\n",
      "INFO:root:0.15254935682\n",
      "INFO:root:0.151722326123\n",
      "INFO:root:0.150936457153\n",
      "INFO:root:0.150188720865\n",
      "INFO:root:0.149476293558\n",
      "INFO:root:0.148796668379\n",
      "INFO:root:0.148147536806\n",
      "INFO:root:0.147526809547\n",
      "INFO:root:0.146932565116\n",
      "INFO:root:0.146363057809\n",
      "INFO:root:0.145816668771\n",
      "INFO:root:0.145292196787\n",
      "INFO:root:0.144788223733\n",
      "INFO:root:0.144303289955\n",
      "INFO:root:0.143836231409\n",
      "INFO:root:0.143385963414\n",
      "INFO:root:0.142951495449\n",
      "INFO:root:0.14253191126\n",
      "INFO:root:0.142126346242\n",
      "INFO:root:0.141734001962\n",
      "INFO:root:0.141354145188\n",
      "INFO:root:0.14098607475\n",
      "INFO:root:0.140629150802\n",
      "INFO:root:0.140283601902\n",
      "INFO:root:0.139948652957\n",
      "INFO:root:0.13962308374\n",
      "INFO:root:0.139306386703\n",
      "INFO:root:0.138998118221\n",
      "INFO:root:0.138697844622\n",
      "INFO:root:0.138405191068\n",
      "INFO:root:train accuracy 0.9833333333333333\n",
      "INFO:root:test accuracy 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlqSK+f/MWjwAi8Az6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a160a6908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test, n_labels = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,n_labels]\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5,  optimize=\"adam\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5, optimize=\"gd\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims,  activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5,   optimize=\"rmsprop\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5, optimize=\"momentum\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
