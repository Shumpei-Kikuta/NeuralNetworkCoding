{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"sigmoid\"\n",
    "        Z, A = forward_function(prev_A, W, b, activation)\n",
    "        \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate):\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, activation=\"relu\", initialize=\"random\", minibatch_size=None):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for i in range(iteration_num):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "        \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            logger.info(cost)\n",
    "            \n",
    "    plt.title(\"Cost Function\")\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"tanh\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train accuracy\n",
      "DEBUG:root:1\n",
      "INFO:root:0.6931356426224851\n",
      "INFO:root:0.32727577579964406\n",
      "INFO:root:0.1290438400744079\n",
      "INFO:root:0.08475627386811052\n",
      "INFO:root:0.06742357306449127\n",
      "INFO:root:0.058570100131629374\n",
      "INFO:root:0.053283919452727715\n",
      "INFO:root:0.049839578674134716\n",
      "INFO:root:0.047429600352152976\n",
      "INFO:root:0.04563862229874341\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 0.5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu5lZATnczcwK6P8Dqy/KAhe4EhoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ff08588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    logger.info(\"train accuracy\")\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5, keep_probs=1)\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1, activation=\"relu\") #\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
