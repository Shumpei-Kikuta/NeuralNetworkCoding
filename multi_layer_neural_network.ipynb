{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  1.25],\n",
       "       [ 2.5 ,  3.75],\n",
       "       [ 5.  ,  6.25]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.random.rand(3, 2)\n",
    "D = (D <  0.8)\n",
    "A = np.arange(6).reshape(3,2)\n",
    "A = D * A\n",
    "A / 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"sigmoid\"\n",
    "            keep_probs = 1\n",
    "            \n",
    "        Z, A = forward_function(prev_A, W, b, activation)    \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, v, S):\n",
    "    \n",
    "    if learning_rate_decay:\n",
    "        learning_rate = (0.99999999 ** iteration_i) * learning_rate\n",
    "        \n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.000001\n",
    "        \n",
    "    if optimize==\"gd\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"momentum\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * v[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * v[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"rmsprop\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)] / np.sqrt(S[\"dW\" + str(i)] + epsilon)\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * S[\"db\" + str(i)] / np.sqrt(S[\"db\" + str(i)] + epsilon)\n",
    "        \n",
    "    elif optimize == \"adam\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            \n",
    "            # adjust the scale of each values\n",
    "            vdW_correction = v[\"dW\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            vdb_correction = v[\"db\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            SdW_correction = S[\"dW\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            Sdb_correction = S[\"db\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            \n",
    "            parameters[\"W\" + str(i)] -= learning_rate * vdW_correction / np.sqrt(SdW_correction + epsilon) \n",
    "            parameters[\"b\" + str(i)] -= learning_rate * vdb_correction / np.sqrt(Sdb_correction + epsilon)\n",
    "            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_momentum(parameters, optimize):\n",
    "    optimize_for_momentums = {}\n",
    "    if optimize == \"gd\" or optimize == \"rmsprop\":\n",
    "        pass\n",
    "    \n",
    "    elif optimize == \"momentum\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_momentums[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "    \n",
    "    \n",
    "    return optimize_for_momentums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_rmsprop(parameters, optimize):\n",
    "    optimize_for_rmsprops = {}\n",
    "    if optimize == \"sgd\" or optimize == \"momentum\":\n",
    "        pass\n",
    "    elif optimize == \"rmsprop\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_rmsprops[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "            \n",
    "    return optimize_for_rmsprops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, \\\n",
    "         activation=\"relu\", initialize=\"random\", minibatch_size=None, learning_rate_decay=False, \\\n",
    "        optimize=\"gd\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    \n",
    "    optimize_for_momentums = initialize_for_momentum(parameters, optimize)\n",
    "    \n",
    "    optimize_for_rmsprops = initialize_for_rmsprop(parameters, optimize)\n",
    "        \n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for iteration_i in range(1, iteration_num + 1):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            \n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "            \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, optimize_for_momentums, optimize_for_rmsprops)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if iteration_i % 100 == 0:\n",
    "            logger.info(cost)\n",
    "            \n",
    "    plt.title(\"{}\".format(optimize))\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"relu\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:1\n",
      "INFO:root:0.05025242735240319\n",
      "INFO:root:0.035923558149411\n",
      "INFO:root:0.033764847707093114\n",
      "INFO:root:0.03256183314107888\n",
      "INFO:root:0.03184140113192871\n",
      "INFO:root:0.03139301124920629\n",
      "INFO:root:0.031107086044067235\n",
      "INFO:root:0.030918462439387326\n",
      "INFO:root:0.030788034820392343\n",
      "INFO:root:0.030693122098354248\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 1.0\n",
      "DEBUG:root:1\n",
      "INFO:root:0.6931524163891376\n",
      "INFO:root:0.6931506025224875\n",
      "INFO:root:0.6931477423738005\n",
      "INFO:root:0.6931353323376767\n",
      "INFO:root:0.6930884781284661\n",
      "INFO:root:0.6929549634548462\n",
      "INFO:root:0.6925422137019012\n",
      "INFO:root:0.6912666105369467\n",
      "INFO:root:0.6873665431465074\n",
      "INFO:root:0.6758405314677386\n",
      "INFO:root:train accuracy 0.825\n",
      "INFO:root:test accuracy 0.85\n",
      "DEBUG:root:1\n",
      "INFO:root:0.693147182013187\n",
      "INFO:root:0.69314718056106\n",
      "INFO:root:0.6931471805622277\n",
      "INFO:root:0.6931471805636571\n",
      "INFO:root:0.6931471805652487\n",
      "INFO:root:0.6931471805669333\n",
      "INFO:root:0.6931471805686583\n",
      "INFO:root:0.6931471805703838\n",
      "INFO:root:0.6931471805720806\n",
      "INFO:root:0.6931471805737274\n",
      "INFO:root:train accuracy 0.5\n",
      "INFO:root:test accuracy 0.5\n",
      "DEBUG:root:1\n",
      "INFO:root:0.6929489654275477\n",
      "INFO:root:0.692392584792129\n",
      "INFO:root:0.6904986386643449\n",
      "INFO:root:0.6837553493648828\n",
      "INFO:root:0.6607173564682826\n",
      "INFO:root:0.5943129141716065\n",
      "INFO:root:0.45113075314045326\n",
      "INFO:root:0.2738471876035758\n",
      "INFO:root:0.15928576204633524\n",
      "INFO:root:0.10394322919694586\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ed2e278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5,  optimize=\"adam\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, optimize=\"gd\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims,  activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5,   optimize=\"rmsprop\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, optimize=\"momentum\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
