{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_iris, prepare_digits, prepare_mnist\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    epsilon = 10 ** (-8)\n",
    "    exp_Z = np.exp(Z)\n",
    "    sum_Z = np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    assert(sum_Z.shape == (1, exp_Z.shape[1]))\n",
    "    return exp_Z / sum_Z + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"softmax\"\n",
    "            keep_probs = 1\n",
    "            \n",
    "        Z, A = forward_function(prev_A, W, b, activation)    \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    epsilon = 10 ** (-8)\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1/m * np.sum(y *np.log(AL + epsilon)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    assert(y.shape == AL.shape)\n",
    "    epsilon = 10 ** (-8)\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / (AL+epsilon)) + (1 - y) / (1 - AL + epsilon)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, v, S):\n",
    "    \n",
    "    if learning_rate_decay:\n",
    "        learning_rate = (0.99999999 ** iteration_i) * learning_rate\n",
    "        \n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.000001\n",
    "        \n",
    "    if optimize==\"gd\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"momentum\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * v[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * v[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"rmsprop\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)] / np.sqrt(S[\"dW\" + str(i)] + epsilon)\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * S[\"db\" + str(i)] / np.sqrt(S[\"db\" + str(i)] + epsilon)\n",
    "        \n",
    "    elif optimize == \"adam\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            \n",
    "            # adjust the scale of each values\n",
    "            vdW_correction = v[\"dW\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            vdb_correction = v[\"db\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            SdW_correction = S[\"dW\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            Sdb_correction = S[\"db\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            \n",
    "            parameters[\"W\" + str(i)] -= learning_rate * vdW_correction / np.sqrt(SdW_correction + epsilon) \n",
    "            parameters[\"b\" + str(i)] -= learning_rate * vdb_correction / np.sqrt(Sdb_correction + epsilon)\n",
    "            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_momentum(parameters, optimize):\n",
    "    optimize_for_momentums = {}\n",
    "    if optimize == \"gd\" or optimize == \"rmsprop\":\n",
    "        pass\n",
    "    \n",
    "    elif optimize == \"momentum\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_momentums[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "    \n",
    "    \n",
    "    return optimize_for_momentums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_rmsprop(parameters, optimize):\n",
    "    optimize_for_rmsprops = {}\n",
    "    if optimize == \"sgd\" or optimize == \"momentum\":\n",
    "        pass\n",
    "    elif optimize == \"rmsprop\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_rmsprops[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "            \n",
    "    return optimize_for_rmsprops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, \\\n",
    "         activation=\"relu\", initialize=\"he\", minibatch_size=None, learning_rate_decay=False, \\\n",
    "        optimize=\"gd\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    \n",
    "    optimize_for_momentums = initialize_for_momentum(parameters, optimize)\n",
    "    \n",
    "    optimize_for_rmsprops = initialize_for_rmsprop(parameters, optimize)\n",
    "        \n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for iteration_i in range(1, iteration_num + 1):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            \n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "            \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, optimize_for_momentums, optimize_for_rmsprops)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if iteration_i % 100 == 0:\n",
    "            logger.info(cost)\n",
    "#             print(outputs[\"A2\"])\n",
    "            \n",
    "    plt.title(\"{}\".format(optimize))\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"relu\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    for i in range(X.shape[1]) :\n",
    "        k = np.argmax(y_hat[:, i])\n",
    "        tmp = np.zeros((y_hat.shape[0]))\n",
    "        tmp[k] = 1\n",
    "        y_hat[:, i] = tmp\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y_hat.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / (y.shape[1] * y.shape[0])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:1\n",
      "INFO:root:train accuracy 0.9034098816979819\n",
      "INFO:root:test accuracy 0.8944444444444445\n",
      "DEBUG:root:1\n",
      "INFO:root:train accuracy 0.8765483646485734\n",
      "INFO:root:test accuracy 0.875\n",
      "DEBUG:root:1\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/shumpei/Desktop/ai_specialization_course/activation_helper.py:9: RuntimeWarning: invalid value encountered in maximum\n",
      "  return np.maximum(0, x)\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in greater\n",
      "INFO:root:train accuracy 0.8210160055671538\n",
      "INFO:root:test accuracy 0.815\n",
      "DEBUG:root:1\n",
      "INFO:root:train accuracy 0.8329853862212944\n",
      "INFO:root:test accuracy 0.8272222222222222\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH0YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3a437a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test, n_labels = prepare_digits()\n",
    "    layer_dims=[X_train.shape[0],40,20,n_labels]\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10, \\\n",
    "                      lambd=0.5,  optimize=\"adam\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10, \\\n",
    "                      lambd=0.5, optimize=\"gd\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims,  activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10, \\\n",
    "                      lambd=0.5,   optimize=\"rmsprop\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10, \\\n",
    "                      lambd=0.5, optimize=\"momentum\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
