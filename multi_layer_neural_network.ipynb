{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'ann', 'a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\"an\": 1,\"ann\":2, \"a\":3}\n",
    "list(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train accuracy\n",
      "DEBUG:root:0.6934288068086597\n",
      "DEBUG:root:0.6563044437263771\n",
      "DEBUG:root:0.31854097957327693\n",
      "DEBUG:root:0.12020496819574897\n",
      "DEBUG:root:0.0833295789360563\n",
      "DEBUG:root:0.0731696102463637\n",
      "DEBUG:root:0.06942577470669323\n",
      "DEBUG:root:0.0678021705110615\n",
      "DEBUG:root:0.06702118957279363\n",
      "DEBUG:root:0.0666155189005324\n",
      "INFO:root:1.0\n",
      "INFO:root:1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a20d649e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "    \n",
    "\n",
    "def initialize_parameter(layer_dims: list) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        # Initialize in the accordance with He papers\n",
    "        # parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n",
    "\n",
    "def forward_function(prev_A, W, b):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    A = sigmoid(Z)    \n",
    "    \n",
    "    #test done\n",
    "    return Z, A\n",
    "    \n",
    "\n",
    "\n",
    "def l_layer_forward(X, parameters, layer_dims, keep_probs):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        Z, A = forward_function(prev_A, W, b)\n",
    "        \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts\n",
    "\n",
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads \n",
    "\n",
    "def backward_function(dA, Z, A, prev_A, W, m, lambd):\n",
    "    # limited sigmoid\n",
    "    dZ = dA * A * (1 - A)\n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A\n",
    "\n",
    "def update_parameters(parameters, grads, layer_dims, learning_rate):\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "\n",
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=100, lambd=0,keep_probs=1):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    parameters = initialize_parameter(layer_dims)\n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    m = X.shape[1]\n",
    "    costs = np.array([])\n",
    "    for i in range(iteration_num):\n",
    "        outputs = {}\n",
    "        outputs, dropouts = l_layer_forward(X, parameters, layer_dims, keep_probs)\n",
    "        cost=caluculate_cost(outputs[str_AL],y,m,lambd,parameters)\n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        grads = {}\n",
    "        grads = l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, layer_dims, learning_rate)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            logger.debug(cost)\n",
    "            \n",
    "    plt.title(\"Cost Function\")\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "    \n",
    "def predict(X, parameters, layer_dims, keep_probs=1):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score\n",
    "    \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],100,1]\n",
    "    logger.info(\"train accuracy\")\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, lambd=0.3, keep_probs=1)\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1) #\n",
    "    logger.info(caluculate_score(y_train, y_train_hat))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(caluculate_score(y_test, y_test_hat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
