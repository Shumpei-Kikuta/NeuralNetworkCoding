{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "    \n",
    "\n",
    "def initialize_parameter(layer_dims: list) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n",
    "\n",
    "def forward_function(prev_A, W, b):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    A = sigmoid(Z)    \n",
    "    \n",
    "    #test done\n",
    "    return Z, A\n",
    "    \n",
    "\n",
    "\n",
    "def l_layer_forward(X, parameters, layer_dims):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        Z, A = forward_function(prev_A, W, b)\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs\n",
    "\n",
    "def caluculate_cost(AL, y, m):\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads \n",
    "\n",
    "def backward_function(dA, Z, A, prev_A, W, m):\n",
    "    # limited sigmoid\n",
    "    dZ = dA * A * (1 - A)\n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T)\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A\n",
    "\n",
    "def update_parameters(parameters, grads, layer_dims, learning_rate):\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "\n",
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=10):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    parameters = initialize_parameter(layer_dims)\n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    m = X.shape[1]\n",
    "    for i in range(iteration_num):\n",
    "        outputs = {}\n",
    "        outputs = l_layer_forward(X, parameters, layer_dims)\n",
    "        cost = caluculate_cost(outputs[str_AL], y, m)\n",
    "        \n",
    "        grads = {}\n",
    "        grads = l_layer_backward(parameters, outputs, grads, layer_dims, y, m)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, layer_dims, learning_rate)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            logger.debug(cost)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    logger.info(\"train accuracy\")\n",
    "    main(X_train, y_train, layer_dims,logger, learning_rate=0.1, iteration_num=10000)\n",
    "    logger.info(\"test accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
