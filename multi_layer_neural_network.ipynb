{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train accuracy\n",
      "DEBUG:root:3.17268153634519\n",
      "DEBUG:root:0.06840905796497926\n",
      "DEBUG:root:0.054605029168574674\n",
      "DEBUG:root:0.05088066512686815\n",
      "DEBUG:root:0.04850069615788128\n",
      "DEBUG:root:0.046581538475412146\n",
      "DEBUG:root:0.04493345701222277\n",
      "DEBUG:root:0.04349136681004676\n",
      "DEBUG:root:0.0422220012474215\n",
      "DEBUG:root:0.04110297547577917\n",
      "INFO:root:1.0\n",
      "INFO:root:0.95\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhwx3Seohw12Seshwl6Qe+v+jyRby0B6w7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c6cacc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "    \n",
    "\n",
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A\n",
    "    \n",
    "\n",
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"sigmoid\"\n",
    "        Z, A = forward_function(prev_A, W, b, activation)\n",
    "        \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts\n",
    "\n",
    "\n",
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "\n",
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads \n",
    "\n",
    "\n",
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, layer_dims, learning_rate):\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "\n",
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=100, lambd=0,keep_probs=1, activation=\"tanh\", initialize=\"random\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    m = X.shape[1]\n",
    "    costs = np.array([])\n",
    "    for i in range(iteration_num):\n",
    "        outputs = {}\n",
    "        outputs, dropouts = l_layer_forward(X, parameters, layer_dims, keep_probs, activation)\n",
    "        cost=caluculate_cost(outputs[str_AL],y,m,lambd,parameters)\n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        grads = {}\n",
    "        grads = l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, layer_dims, learning_rate)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            logger.debug(cost)\n",
    "            \n",
    "    plt.title(\"Cost Function\")\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "    \n",
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"tanh\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score\n",
    "    \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    logger.info(\"train accuracy\")\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, lambd=0.5, keep_probs=1,activation=\"relu\", initialize=\"he\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1, activation=\"relu\") #\n",
    "    logger.info(caluculate_score(y_train, y_train_hat))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(caluculate_score(y_test, y_test_hat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
