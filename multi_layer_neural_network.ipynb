{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_iris, prepare_digits\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z)\n",
    "    sum_Z = np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    assert(sum_Z.shape == (1, exp_Z.shape[1]))\n",
    "    return exp_Z / sum_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"softmax\"\n",
    "            keep_probs = 1\n",
    "            \n",
    "        Z, A = forward_function(prev_A, W, b, activation)    \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1/m * np.sum(y *np.log(AL)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    assert(y.shape == AL.shape)\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, v, S):\n",
    "    \n",
    "    if learning_rate_decay:\n",
    "        learning_rate = (0.99999999 ** iteration_i) * learning_rate\n",
    "        \n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.000001\n",
    "        \n",
    "    if optimize==\"gd\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"momentum\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * v[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * v[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"rmsprop\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)] / np.sqrt(S[\"dW\" + str(i)] + epsilon)\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * S[\"db\" + str(i)] / np.sqrt(S[\"db\" + str(i)] + epsilon)\n",
    "        \n",
    "    elif optimize == \"adam\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            \n",
    "            # adjust the scale of each values\n",
    "            vdW_correction = v[\"dW\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            vdb_correction = v[\"db\" + str(i)] / (1 - beta1 ** iteration_i)\n",
    "            SdW_correction = S[\"dW\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            Sdb_correction = S[\"db\" + str(i)] / (1 - beta2 ** iteration_i)\n",
    "            \n",
    "            parameters[\"W\" + str(i)] -= learning_rate * vdW_correction / np.sqrt(SdW_correction + epsilon) \n",
    "            parameters[\"b\" + str(i)] -= learning_rate * vdb_correction / np.sqrt(Sdb_correction + epsilon)\n",
    "            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_momentum(parameters, optimize):\n",
    "    optimize_for_momentums = {}\n",
    "    if optimize == \"gd\" or optimize == \"rmsprop\":\n",
    "        pass\n",
    "    \n",
    "    elif optimize == \"momentum\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_momentums[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "    \n",
    "    \n",
    "    return optimize_for_momentums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_rmsprop(parameters, optimize):\n",
    "    optimize_for_rmsprops = {}\n",
    "    if optimize == \"sgd\" or optimize == \"momentum\":\n",
    "        pass\n",
    "    elif optimize == \"rmsprop\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_rmsprops[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "            \n",
    "    return optimize_for_rmsprops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, \\\n",
    "         activation=\"relu\", initialize=\"he\", minibatch_size=None, learning_rate_decay=False, \\\n",
    "        optimize=\"gd\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    \n",
    "    optimize_for_momentums = initialize_for_momentum(parameters, optimize)\n",
    "    \n",
    "    optimize_for_rmsprops = initialize_for_rmsprop(parameters, optimize)\n",
    "        \n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for iteration_i in range(1, iteration_num + 1):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            \n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "            \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, iteration_i, optimize, optimize_for_momentums, optimize_for_rmsprops)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if iteration_i % 100 == 0:\n",
    "            logger.info(cost)\n",
    "#             print(outputs[\"A2\"])\n",
    "            \n",
    "    plt.title(\"{}\".format(optimize))\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"relu\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    for i in range(X.shape[1]) :\n",
    "        k = np.argmax(y_hat[:, i])\n",
    "        tmp = np.zeros((y_hat.shape[0]))\n",
    "        tmp[k] = 1\n",
    "        y_hat[:, i] = tmp\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y_hat.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / (y.shape[1] * y.shape[0])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:1\n",
      "INFO:root:0.0785048546632\n",
      "INFO:root:0.0352040759979\n",
      "INFO:root:0.0268933606623\n",
      "INFO:root:0.0237200948856\n",
      "INFO:root:0.0219668159622\n",
      "INFO:root:0.0206995054262\n",
      "INFO:root:0.0196900025719\n",
      "INFO:root:0.0188425937221\n",
      "INFO:root:0.0180929353391\n",
      "INFO:root:0.0174094553297\n",
      "INFO:root:0.016794242572\n",
      "INFO:root:0.0162250545164\n",
      "INFO:root:0.0156840858745\n",
      "INFO:root:0.0151760408145\n",
      "INFO:root:0.0147007110281\n",
      "INFO:root:0.0142568530089\n",
      "INFO:root:0.0138351412905\n",
      "INFO:root:0.013433577565\n",
      "INFO:root:0.0130496655348\n",
      "INFO:root:0.0126783753245\n",
      "INFO:root:0.0123219542214\n",
      "INFO:root:0.0119744710333\n",
      "INFO:root:0.0116469030355\n",
      "INFO:root:0.0113356856338\n",
      "INFO:root:0.0110383186276\n",
      "INFO:root:0.0107543613796\n",
      "INFO:root:0.0104810728021\n",
      "INFO:root:0.0102208543302\n",
      "INFO:root:0.00996791169851\n",
      "INFO:root:0.00971758014171\n",
      "INFO:root:0.00948009560743\n",
      "INFO:root:0.00925519588861\n",
      "INFO:root:0.00904042510309\n",
      "INFO:root:0.00883808609966\n",
      "INFO:root:0.00864530821387\n",
      "INFO:root:0.00846032823331\n",
      "INFO:root:0.00828506906371\n",
      "INFO:root:0.00811962334013\n",
      "INFO:root:0.00796319705162\n",
      "INFO:root:0.00781627017121\n",
      "INFO:root:0.0076782917029\n",
      "INFO:root:0.00754971228289\n",
      "INFO:root:0.00742850052719\n",
      "INFO:root:0.00731638961081\n",
      "INFO:root:0.00721250071384\n",
      "INFO:root:0.00711500311981\n",
      "INFO:root:0.00702277041646\n",
      "INFO:root:0.00693816603774\n",
      "INFO:root:0.00685719741618\n",
      "INFO:root:0.00678249118845\n",
      "INFO:root:0.0067134578273\n",
      "INFO:root:0.0066498562171\n",
      "INFO:root:0.00659174037423\n",
      "INFO:root:0.00653895402073\n",
      "INFO:root:0.00648957718613\n",
      "INFO:root:0.00644382021923\n",
      "INFO:root:0.00640128430523\n",
      "INFO:root:0.00636264117117\n",
      "INFO:root:0.00632688239669\n",
      "INFO:root:0.00629410991384\n",
      "INFO:root:0.00626372369829\n",
      "INFO:root:0.00623571645163\n",
      "INFO:root:0.00620997627665\n",
      "INFO:root:0.00618660173272\n",
      "INFO:root:0.00616464858199\n",
      "INFO:root:0.00614421427645\n",
      "INFO:root:0.00612455369652\n",
      "INFO:root:0.00610718878699\n",
      "INFO:root:0.00609168012455\n",
      "INFO:root:0.00607662539052\n",
      "INFO:root:0.00606319017729\n",
      "INFO:root:0.00604991063786\n",
      "INFO:root:0.00603683601423\n",
      "INFO:root:0.00602473293599\n",
      "INFO:root:0.00601197335323\n",
      "INFO:root:0.006000349494\n",
      "INFO:root:0.0059890821011\n",
      "INFO:root:0.00597954929771\n",
      "INFO:root:0.00597083499737\n",
      "INFO:root:0.0059627850232\n",
      "INFO:root:0.00595549189355\n",
      "INFO:root:0.00594903378518\n",
      "INFO:root:0.00594226299983\n",
      "INFO:root:0.00593604130003\n",
      "INFO:root:0.00592940724117\n",
      "INFO:root:0.0059235686525\n",
      "INFO:root:0.00591763922244\n",
      "INFO:root:0.00591216190324\n",
      "INFO:root:0.00590682681241\n",
      "INFO:root:0.00590218078732\n",
      "INFO:root:0.00589766549104\n",
      "INFO:root:0.0058933330283\n",
      "INFO:root:0.00588871302158\n",
      "INFO:root:0.00588486969356\n",
      "INFO:root:0.00588112434871\n",
      "INFO:root:0.00587713529083\n",
      "INFO:root:0.00587381132306\n",
      "INFO:root:0.00587068020254\n",
      "INFO:root:0.00586719231678\n",
      "INFO:root:0.00586363510297\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 0.9955555555555555\n",
      "DEBUG:root:1\n",
      "INFO:root:1.35034758081\n",
      "INFO:root:0.874391040487\n",
      "INFO:root:0.483538744891\n",
      "INFO:root:0.273934536758\n",
      "INFO:root:0.196424230884\n",
      "INFO:root:0.152745229788\n",
      "INFO:root:0.125693847866\n",
      "INFO:root:0.107937590974\n",
      "INFO:root:0.094970714198\n",
      "INFO:root:0.0851443158744\n",
      "INFO:root:train accuracy 0.9977731384829506\n",
      "INFO:root:test accuracy 0.9883333333333333\n",
      "DEBUG:root:1\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: RuntimeWarning: overflow encountered in true_divide\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  app.launch_new_instance()\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "/Users/shumpei/Desktop/ai_specialization_course/activation_helper.py:9: RuntimeWarning: invalid value encountered in maximum\n",
      "  return np.maximum(0, x)\n",
      "/Users/shumpei/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in greater\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:nan\n",
      "INFO:root:train accuracy 0.8210160055671538\n",
      "INFO:root:test accuracy 0.815\n",
      "DEBUG:root:1\n",
      "INFO:root:1.23779386249\n",
      "INFO:root:0.706644262473\n",
      "INFO:root:0.384396831381\n",
      "INFO:root:0.278926582758\n",
      "INFO:root:0.225075715705\n",
      "INFO:root:0.189254244654\n",
      "INFO:root:0.164515849785\n",
      "INFO:root:0.146521070176\n",
      "INFO:root:0.132705787541\n",
      "INFO:root:0.121413452551\n",
      "INFO:root:train accuracy 0.9962421711899792\n",
      "INFO:root:test accuracy 0.9872222222222222\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngjDQJakgDHRJKoj/D9yLATbqZhbWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1fa99ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test, n_labels = prepare_digits()\n",
    "    layer_dims=[X_train.shape[0],40,20,n_labels]\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5,  optimize=\"adam\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, optimize=\"gd\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims,  activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5,   optimize=\"rmsprop\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    \n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=1000, \\\n",
    "                      lambd=0.5, optimize=\"momentum\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, activation=\"relu\")\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
