{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_helper import sigmoid, relu\n",
    "from make_dataset_helper import prepare_dataset\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_function(prev_A, W, b, activation):\n",
    "    Z = np.dot(W, prev_A) + b\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)    \n",
    "    elif activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    #test done\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forward(X, parameters, layer_dims, keep_probs, activation):\n",
    "    layer_num = len(layer_dims)\n",
    "    outputs = {}\n",
    "    dropouts = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, layer_num):\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        if i == layer_num - 1:\n",
    "            activation = \"sigmoid\"\n",
    "        Z, A = forward_function(prev_A, W, b, activation)\n",
    "        \n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D <  keep_probs)\n",
    "        A = D * A\n",
    "        A = A / keep_probs\n",
    "        outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)] = Z, A\n",
    "        dropouts[\"D\" + str(i)] = D\n",
    "        \n",
    "        assert(outputs[\"Z\" + str(i)].shape == outputs[\"A\" + str(i)].shape)\n",
    "        assert(outputs[\"Z\" + str(i)].shape == (layer_dims[i], X.shape[1]))\n",
    "    \n",
    "    # test done\n",
    "    return outputs, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_cost(AL, y, m, lambd, parameters):\n",
    "    numW = len(parameters) // 2\n",
    "    squared_sumlistW = np.array([])\n",
    "    for i in range(1, numW + 1):\n",
    "        squared_sumlistW = np.append(squared_sumlistW, np.sum(parameters[\"W\" + str(i)] ** 2))\n",
    "    cost = - 1 / m * (np.dot(y, np.log(AL.T)) + np.dot((1 - y), np.log(1 - AL).T)) + (1/m) * (lambd / 2) * (np.sum(squared_sumlistW))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_layer_backward(parameters, outputs, grads, layer_dims, y, m, lambd, dropouts, keep_probs, activation) -> dict:\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = outputs[\"A\" + str(layer_num - 1)]\n",
    "    grads[\"dA\" + str(layer_num - 1)] = -(y / AL) + (1 - y) / (1 - AL)\n",
    "    for i in reversed(range(1, layer_num)):\n",
    "        Z, A = outputs[\"Z\" + str(i)], outputs[\"A\" + str(i)]\n",
    "        prev_A = outputs[\"A\" + str(i - 1)]\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        dA = grads[\"dA\" + str(i)]\n",
    "        D  = dropouts[\"D\" + str(i)]\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_probs\n",
    "        if i == layer_num -1:\n",
    "            activation_f = \"sigmoid\"\n",
    "        else:\n",
    "            activation_f = activation\n",
    "            \n",
    "        dZ, dW, db, dprev_A = backward_function(dA, Z, A, prev_A, W, m, lambd, activation_f)\n",
    "        grads[\"dW\" + str(i)], grads[\"db\" + str(i)], grads[\"dA\" + str(i - 1)] = dW, db, dprev_A\n",
    "        assert(Z.shape == dZ.shape)\n",
    "        assert(W.shape == dW.shape)\n",
    "        \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_function(dA, Z, A, prev_A, W, m, lambd, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * A * (1 - A)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA * (1 - A ** 2)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        temp = (Z > 0)\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    dW = 1 / m * np.dot(dZ, prev_A.T) + lambd / m * W\n",
    "    db = 1/ m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dprev_A = np.dot(W.T, dZ)\n",
    "    return dZ, dW, db, dprev_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameter(layer_dims: list, initialize: str) -> dict:\n",
    "    # generate parameter W, b\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialize == \"random\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        elif initialize == \"he\":\n",
    "            parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "            \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    assert(parameters[\"W1\"].shape == (layer_dims[1], layer_dims[0]))\n",
    "    \n",
    "    # test done\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, layer_dims, learning_rate0, learning_rate_decay, iteration_i, optimize, v, S):\n",
    "    if learning_rate_decay:\n",
    "        learning_rate = (0.99999999 ** iteration_i) * learning_rate0\n",
    "        \n",
    "    if optimize==\"gd\":\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"momentum\":\n",
    "        beta1 = 0.9\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            v[\"dW\" + str(i)] = beta1 * v[\"dW\" + str(i)] + (1 - beta1) * grads[\"dW\" + str(i)]\n",
    "            v[\"db\" + str(i)] = beta1 * v[\"db\" + str(i)] + (1 - beta1) * grads[\"db\"+ str(i)]\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * v[\"dW\" + str(i)]\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * v[\"db\" + str(i)]\n",
    "            \n",
    "    elif optimize == \"rmsprop\":\n",
    "        beta2=0.9\n",
    "        epsilon = 0.000000001\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            S[\"dW\" + str(i)] = beta2 * S[\"dW\" + str(i)] + (1 - beta2) * grads[\"dW\" + str(i)]**2\n",
    "            S[\"db\" + str(i)] = beta2 * S[\"db\" + str(i)] + (1 - beta2) * grads[\"db\"+ str(i)]**2\n",
    "            parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)] / np.sqrt(S[\"dW\" + str(i)] + epsilon)\n",
    "            parameters[\"b\" + str(i)] -= learning_rate * S[\"db\" + str(i)] / np.sqrt(S[\"db\" + str(i)] + epsilon)\n",
    "        \n",
    "    elif optimize == \"adam\":\n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_momentum(parameters, optimize):\n",
    "    optimize_for_momentums = {}\n",
    "    if optimize == \"gd\" or optimize == \"rmsprop\":\n",
    "        pass\n",
    "    \n",
    "    elif optimize == \"momentum\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_momentums[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "    \n",
    "    \n",
    "    return optimize_for_momentums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_for_rmsprop(parameters, optimize):\n",
    "    optimize_for_rmsprops = {}\n",
    "    if optimize == \"sgd\" or optimize == \"momentum\":\n",
    "        pass\n",
    "    elif optimize == \"rmsprop\" or optimize == \"adam\":\n",
    "        for key in parameters.keys():\n",
    "            optimize_for_rmsprops[\"d\" + str(key)] = np.zeros(parameters[key].shape)\n",
    "            \n",
    "    return optimize_for_rmsprops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(X, y, layer_dims, logger, learning_rate=0.01, iteration_num=1000, lambd=0,keep_probs=1, \\\n",
    "         activation=\"relu\", initialize=\"random\", minibatch_size=None, learning_rate_decay=False, \\\n",
    "        optimize=\"gd\"):\n",
    "    # main function\n",
    "    # valuable \n",
    "    # parameters - contain W and b on each layer as dictionary like \"W1\"\n",
    "    # outputs - coutain Z and A on each layer as dictionary like \"A1\"\n",
    "    if minibatch_size==None:\n",
    "        minibatch_size = X.shape[1]\n",
    "    \n",
    "#     generate_minibatch(X, y, minibatch_size)\n",
    "    minibatch_num = math.ceil(X.shape[1] / minibatch_size)\n",
    "    logger.debug(minibatch_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameter(layer_dims, initialize)\n",
    "    \n",
    "    optimize_for_momentums = initialize_for_momentum(parameters, optimize)\n",
    "    \n",
    "    optimize_for_rmsprops = initialize_for_rmsprop(parameters, optimize)\n",
    "        \n",
    "    str_AL = \"A\" + str(len(layer_dims) - 1)\n",
    "    costs = np.array([])\n",
    "    for i in range(iteration_num):\n",
    "        partition = list(np.random.permutation(X.shape[1]))\n",
    "        X = X[:, partition]\n",
    "        y = y[:, partition]\n",
    "        for k in range(minibatch_num):\n",
    "            if k == minibatch_num - 1:\n",
    "                # final minibatch\n",
    "                X_shuffle = X[:, k * minibatch_size:]\n",
    "                y_shuffle = y[:, k* minibatch_size:]\n",
    "            else:\n",
    "                X_shuffle = X[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "                y_shuffle = y[:, k * minibatch_size: (k + 1) * minibatch_size]\n",
    "            m = X_shuffle.shape[1]\n",
    "            outputs, dropouts = l_layer_forward(X_shuffle, parameters, layer_dims, keep_probs, activation)\n",
    "            cost=caluculate_cost(outputs[str_AL],y_shuffle,m,lambd,parameters)\n",
    "        \n",
    "            grads = {}\n",
    "            grads = l_layer_backward(parameters, outputs, grads, layer_dims, y_shuffle, m, lambd, dropouts, keep_probs, activation)\n",
    "        \n",
    "            parameters = update_parameters(parameters, grads, layer_dims, learning_rate, learning_rate_decay, i, optimize, optimize_for_momentums, optimize_for_rmsprops)\n",
    "        \n",
    "        costs = np.append(costs, cost)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            logger.info(cost)\n",
    "            \n",
    "    plt.title(\"Cost Function\")\n",
    "    plt.scatter(x=range(iteration_num),y=costs)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters, layer_dims, keep_probs=1, activation=\"tanh\"):\n",
    "    outputs= l_layer_forward(X, parameters, layer_dims, keep_probs, activation)[0]\n",
    "    layer_num = len(layer_dims)\n",
    "    AL = \"A\" + str(layer_num - 1)\n",
    "    y_hat = outputs[AL]\n",
    "    func = lambda x: 0 if x <= 0.5 else 1\n",
    "    vfunc = np.vectorize(func)\n",
    "    y_hat = vfunc(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caluculate_score(y, y_hat) -> int:\n",
    "    assert(y.shape == y_hat.shape)\n",
    "    score = np.sum(y == y_hat) / y.shape[1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:train accuracy\n",
      "DEBUG:root:1\n",
      "INFO:root:0.6931926430594605\n",
      "INFO:root:0.6928951023574502\n",
      "INFO:root:0.6918544476929441\n",
      "INFO:root:0.687274403052055\n",
      "INFO:root:0.6696985012002322\n",
      "INFO:root:0.6186195494581993\n",
      "INFO:root:0.5097651520875107\n",
      "INFO:root:0.40300945966445406\n",
      "INFO:root:0.3310404387913285\n",
      "INFO:root:0.2826942690008602\n",
      "INFO:root:0.24696106234688023\n",
      "INFO:root:0.21899666671054013\n",
      "INFO:root:0.19644242207893703\n",
      "INFO:root:0.17808789365683841\n",
      "INFO:root:0.16286880749621888\n",
      "INFO:root:0.1500578414705743\n",
      "INFO:root:0.13918728193433985\n",
      "INFO:root:0.12987109966052038\n",
      "INFO:root:0.12181990157050958\n",
      "INFO:root:0.11480865908121143\n",
      "INFO:root:0.10866003060870483\n",
      "INFO:root:0.10323430177349949\n",
      "INFO:root:0.09841875646391854\n",
      "INFO:root:0.09412208330257951\n",
      "INFO:root:0.09026970756471639\n",
      "INFO:root:0.08680015493425153\n",
      "INFO:root:0.08366227545911382\n",
      "INFO:root:0.08081323999262216\n",
      "INFO:root:0.07821701892150676\n",
      "INFO:root:0.07584324750054147\n",
      "INFO:root:0.07366600147473806\n",
      "INFO:root:0.07166321168636364\n",
      "INFO:root:0.06981570882731275\n",
      "INFO:root:0.06810715898769712\n",
      "INFO:root:0.06652313843826876\n",
      "INFO:root:0.06505114909009929\n",
      "INFO:root:0.06368060274372034\n",
      "INFO:root:0.062401581135940444\n",
      "INFO:root:0.06120600225544734\n",
      "INFO:root:0.06008600189634894\n",
      "INFO:root:0.059035359723644265\n",
      "INFO:root:0.05804764694966605\n",
      "INFO:root:0.05711827431779642\n",
      "INFO:root:0.05624201008359617\n",
      "INFO:root:0.05541474592321391\n",
      "INFO:root:0.054632830686231294\n",
      "INFO:root:0.053892726153681715\n",
      "INFO:root:0.05319130296409584\n",
      "INFO:root:0.05252537345329271\n",
      "INFO:root:0.051892226199004646\n",
      "INFO:root:0.051289493884151806\n",
      "INFO:root:0.0507150362140087\n",
      "INFO:root:0.050166907190715794\n",
      "INFO:root:0.04964342701307273\n",
      "INFO:root:0.04914375970358442\n",
      "INFO:root:0.048666380054739666\n",
      "INFO:root:0.04821003276840266\n",
      "INFO:root:0.04777326279064546\n",
      "INFO:root:0.04735490799690535\n",
      "INFO:root:0.046954010008738487\n",
      "INFO:root:0.046569424918894535\n",
      "INFO:root:0.046200316561810964\n",
      "INFO:root:0.04584578328099183\n",
      "INFO:root:0.04550497014570746\n",
      "INFO:root:0.04517688358456931\n",
      "INFO:root:0.044861157893737805\n",
      "INFO:root:0.04455716736292133\n",
      "INFO:root:0.04426430392261543\n",
      "INFO:root:0.043982021139547425\n",
      "INFO:root:0.04370982635959082\n",
      "INFO:root:0.04344714522825491\n",
      "INFO:root:0.0431934919159786\n",
      "INFO:root:0.04294856755063078\n",
      "INFO:root:0.04271185699196905\n",
      "INFO:root:0.04248302290331522\n",
      "INFO:root:0.042261666315147356\n",
      "INFO:root:0.04204733671502023\n",
      "INFO:root:0.04183951325367267\n",
      "INFO:root:0.04163784916139086\n",
      "INFO:root:0.041442045516560895\n",
      "INFO:root:0.04125182640298179\n",
      "INFO:root:0.04106693448814442\n",
      "INFO:root:0.040887129169484976\n",
      "INFO:root:0.04071218515810819\n",
      "INFO:root:0.040541891231650404\n",
      "INFO:root:0.04037604911142427\n",
      "INFO:root:0.040214472447099685\n",
      "INFO:root:0.04005698589683264\n",
      "INFO:root:0.03990342429250044\n",
      "INFO:root:0.03975363188097143\n",
      "INFO:root:0.03960746163342075\n",
      "INFO:root:0.039464774615646764\n",
      "INFO:root:0.039325439413163785\n",
      "INFO:root:0.039189331605563225\n",
      "INFO:root:0.039056333285257636\n",
      "INFO:root:0.03892633261627257\n",
      "INFO:root:0.038799223429223866\n",
      "INFO:root:0.038674904849043706\n",
      "INFO:root:0.038553280952383355\n",
      "INFO:root:0.03843426045194792\n",
      "INFO:root:train accuracy 1.0\n",
      "INFO:root:test accuracy 0.5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyOFuZlZADnczswJyuJuZFdD/A4ufrsHMMtWcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e1305c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(level=\"DEBUG\")\n",
    "    X_train, X_test, y_train, y_test = prepare_dataset()\n",
    "    layer_dims=[X_train.shape[0],3,1]\n",
    "    logger.info(\"train accuracy\")\n",
    "    parameters = main(X_train, y_train, layer_dims,logger, learning_rate=0.01, iteration_num=10000, \\\n",
    "                      lambd=0.5, keep_probs=1, learning_rate_decay=True, optimize=\"gd\")\n",
    "    y_train_hat = predict(X_train, parameters, layer_dims, keep_probs=1, activation=\"relu\") #\n",
    "    logger.info(\"train accuracy {}\".format(caluculate_score(y_train, y_train_hat)))\n",
    "    y_test_hat = predict(X_test, parameters, layer_dims, keep_probs=1)\n",
    "    logger.info(\"test accuracy {}\".format(caluculate_score(y_test, y_test_hat)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
