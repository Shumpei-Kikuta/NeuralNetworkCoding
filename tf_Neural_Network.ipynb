{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-01 10:55:45,382\tINFO\t__main__\tinitialize the computation graph\n",
      "2018-10-01 10:55:45,384\tINFO\t__main__\tcreate X, y placeholder\n",
      "2018-10-01 10:55:45,389\tINFO\t__main__\tintialize_parameter\n",
      "2018-10-01 10:55:45,450\tINFO\t__main__\tforward propagation\n",
      "2018-10-01 10:55:45,459\tINFO\t__main__\tcompute cost function\n",
      "2018-10-01 10:55:45,536\tINFO\t__main__\toptimize with optimizer\n",
      "2018-10-01 10:55:45,780\tINFO\t__main__\tBegin optimizing cost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10 2.5693407\n",
      "epoch20 2.0060406\n",
      "epoch30 1.9772935\n",
      "epoch40 1.9243577\n",
      "epoch50 1.8615102\n",
      "epoch60 1.8052714\n",
      "epoch70 1.7704896\n",
      "epoch80 1.7391356\n",
      "epoch90 1.7081258\n",
      "epoch100 1.6812232\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'costs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-11abfb3a4bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mn_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-11abfb3a4bc5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_logger, X_train, X_test, y_train, y_test, n_labels, n_dims, learning_rate, iteration_num, cost_print)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# plot the cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iterations (per tens)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'costs' is not defined"
     ]
    }
   ],
   "source": [
    "from make_dataset_helper import prepare_mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "def create_placeholder(X, y):\n",
    "    X = tf.placeholder(name=\"X\", dtype=tf.float32, shape=(None, X.shape[1]))\n",
    "    y = tf.placeholder(name=\"y\", dtype=tf.float32, shape=(None, y.shape[1]))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def initialize_parameter(n_dims):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(n_dims)):\n",
    "        parameters[\"W\" + str(i)] = tf.get_variable(name=\"W\"+str(i), shape=(n_dims[i], n_dims[i-1]), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        parameters[\"b\" + str(i)] = tf.get_variable(name=\"b\" + str(i), shape=(n_dims[i], 1), initializer=tf.zeros_initializer())\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_function(parameters, X, n_dims):\n",
    "    outputs = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    for i in range(1, len(n_dims)):\n",
    "        Z = tf.add(tf.matmul(parameters[\"W\" + str(i)], outputs[\"A\" + str(i - 1)]), parameters[\"b\" + str(i)])\n",
    "        A = tf.nn.relu(Z)\n",
    "        outputs[\"Z\" + str(i)] = Z\n",
    "        if i ==len(n_dims) - 1:\n",
    "            continue\n",
    "        outputs[\"A\" + str(i)] = A\n",
    "    return Z\n",
    "        \n",
    "\n",
    "def compute_cost(ZL, y):\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = tf.transpose(y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    return cost\n",
    "        \n",
    "\n",
    "def main(_logger, X_train, X_test, y_train, y_test,  n_labels, n_dims, learning_rate=0.01, iteration_num=100, cost_print=True):\n",
    "    _logger.info(\"initialize the computation graph\")\n",
    "    tf.reset_default_graph()   \n",
    "        \n",
    "    _logger.info(\"create X, y placeholder\")\n",
    "    X, y = create_placeholder(X_train, y_train)\n",
    "    \n",
    "    _logger.info(\"intialize_parameter\")\n",
    "    parameters = initialize_parameter(n_dims)\n",
    "    \n",
    "    _logger.info(\"forward propagation\")\n",
    "    ZL = forward_function(parameters, X, n_dims)\n",
    "    \n",
    "    _logger.info(\"compute cost function\")\n",
    "    cost = compute_cost(ZL, y)\n",
    "    \n",
    "    _logger.info(\"optimize with optimizer\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # initialize all valuables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        _logger.info(\"Begin optimizing cost\")\n",
    "        for epoch in range(1, iteration_num+1):\n",
    "            cost_val, _ = sess.run([cost, optimizer],feed_dict={X: X_train, y: y_train})\n",
    "            if cost_print==True and epoch % 10 == 0:\n",
    "                print(\"epoch\" + str(epoch), cost_val)\n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        _logger.info(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, y: y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, y: y_test})\n",
    "        \n",
    "        _logger.info(\"Train Accuracy: {}\".format(train_accuracy))\n",
    "        _logger.info(\"Test Accuracy: {}\".format(test_accuracy))\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s\\t%(levelname)s\\t%(name)s\\t%(message)s\"\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test, n_labels = prepare_mnist()\n",
    "    n_dims = [X_train.shape[0], 30, n_labels]\n",
    "    main(_logger, X_train, X_test, y_train, y_test,  n_labels, n_dims, learning_rate=0.01, iteration_num=100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-01 11:12:04,931\tINFO\t__main__\tinitialize the computation graph\n",
      "2018-10-01 11:12:04,932\tINFO\t__main__\tcreate X, y placeholder\n",
      "2018-10-01 11:12:04,936\tINFO\t__main__\tintialize_parameter\n",
      "2018-10-01 11:12:05,006\tINFO\t__main__\tforward propagation\n",
      "2018-10-01 11:12:05,009\tINFO\t__main__\tActivation function is relu\n",
      "2018-10-01 11:12:05,021\tINFO\t__main__\tcompute cost function\n",
      "2018-10-01 11:12:05,147\tINFO\t__main__\toptimize with optimizer\n",
      "2018-10-01 11:12:05,505\tINFO\t__main__\tBegin optimizing cost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10 2.3682578\n",
      "epoch20 2.0907528\n",
      "epoch30 2.027593\n",
      "epoch40 1.9643722\n",
      "epoch50 1.9159491\n",
      "epoch60 1.8703178\n"
     ]
    }
   ],
   "source": [
    "from make_dataset_helper import prepare_mnist, prepare_iris, prepare_digits\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "def create_placeholder(X, y):\n",
    "    X = tf.placeholder(name=\"X\", dtype=tf.float32, shape=(X.shape[0], None))\n",
    "    y = tf.placeholder(name=\"y\", dtype=tf.float32, shape=(y.shape[0], None))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def initialize_parameter(n_dims):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(n_dims)):\n",
    "        parameters[\"W\" + str(i)] = tf.get_variable(name=\"W\"+str(i), shape=(n_dims[i], n_dims[i-1]), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        parameters[\"b\" + str(i)] = tf.get_variable(name=\"b\" + str(i), shape=(n_dims[i], 1), initializer=tf.zeros_initializer())\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_function(_logger, parameters, X, n_dims, activation):\n",
    "    outputs = {}\n",
    "    outputs[\"A0\"] = X\n",
    "    _logger.info(\"Activation function is {}\".format(activation))\n",
    "    for i in range(1, len(n_dims)):\n",
    "        Z = tf.add(tf.matmul(parameters[\"W\" + str(i)], outputs[\"A\" + str(i - 1)]), parameters[\"b\" + str(i)])\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            A = tf.nn.relu(Z)\n",
    "            \n",
    "        elif activation == \"sigmoid\":\n",
    "            A = tf.nn.sigmoid(Z)\n",
    "            \n",
    "        elif activation == \"tanh\":\n",
    "            A = tf.nn.tanh(Z)\n",
    "            \n",
    "        outputs[\"Z\" + str(i)] = Z\n",
    "        if i ==len(n_dims) - 1:\n",
    "            continue\n",
    "        outputs[\"A\" + str(i)] = A\n",
    "    return Z\n",
    "        \n",
    "\n",
    "def compute_cost(ZL, y):\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = tf.transpose(y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    return cost\n",
    "        \n",
    "\n",
    "def main(_logger, X_train, X_test, y_train, y_test,  n_labels, n_dims, learning_rate=0.01, iteration_num=100, cost_print=True, activation=\"relu\"):\n",
    "    _logger.info(\"initialize the computation graph\")\n",
    "    tf.reset_default_graph()   \n",
    "        \n",
    "    _logger.info(\"create X, y placeholder\")\n",
    "    X, y = create_placeholder(X_train, y_train)\n",
    "    \n",
    "    _logger.info(\"intialize_parameter\")\n",
    "    parameters = initialize_parameter(n_dims)\n",
    "    \n",
    "    _logger.info(\"forward propagation\")\n",
    "    ZL = forward_function(_logger, parameters, X, n_dims, activation)\n",
    "    \n",
    "    _logger.info(\"compute cost function\")\n",
    "    cost = compute_cost(ZL, y)\n",
    "    \n",
    "    _logger.info(\"optimize with optimizer\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # initialize all valuables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        _logger.info(\"Begin optimizing cost\")\n",
    "        costs = []\n",
    "        \n",
    "        for epoch in range(1, iteration_num+1):\n",
    "            cost_val, _ = sess.run([cost, optimizer],feed_dict={X: X_train, y: y_train})\n",
    "            costs.append(cost_val)\n",
    "            if cost_print==True and epoch % 10 == 0:\n",
    "                print(\"epoch\" + str(epoch), cost_val)\n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        _logger.info(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(ZL), tf.argmax(y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, y: y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, y: y_test})\n",
    "        \n",
    "        _logger.info(\"Train Accuracy: {}\".format(train_accuracy))\n",
    "        _logger.info(\"Test Accuracy: {}\".format(test_accuracy))\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s\\t%(levelname)s\\t%(name)s\\t%(message)s\"\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test, n_labels = prepare_mnist()\n",
    "    n_dims = [X_train.shape[0], 30, n_labels]\n",
    "    main(_logger, X_train, X_test, y_train, y_test,  n_labels, n_dims, learning_rate=0.01, iteration_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
